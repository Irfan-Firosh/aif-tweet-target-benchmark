[
  {
    "model": "llama3.3:70b",
    "n_shots": 0,
    "target_macro_f1": 0.4408864873555826,
    "target_precision": 0.44447761194029856,
    "target_recall": 0.4698482687070419,
    "target_accuracy": 0.6174430128840436,
    "sentiment_macro_f1": 0.10147414930396083,
    "sentiment_precision": 0.3432748944451201,
    "sentiment_recall": 0.22496753246753248,
    "sentiment_accuracy": 0.16848364717542122,
    "total_examples": 1009
  },
  {
    "model": "llama3.3:70b",
    "n_shots": 1,
    "target_macro_f1": 0.4062774071384021,
    "target_precision": 0.38715460718885375,
    "target_recall": 0.46428803008688885,
    "target_accuracy": 0.6362735381565907,
    "sentiment_macro_f1": 0.1260022640741176,
    "sentiment_precision": 0.3925627404672223,
    "sentiment_recall": 0.24530471821756225,
    "sentiment_accuracy": 0.17938553022794845,
    "total_examples": 1009
  },
  {
    "model": "llama3.3:70b",
    "n_shots": 3,
    "target_macro_f1": 0.43010552859037704,
    "target_precision": 0.5671890735146022,
    "target_recall": 0.5049442355077163,
    "target_accuracy": 0.6927651139742319,
    "sentiment_macro_f1": 0.2698959375438454,
    "sentiment_precision": 0.41364221976394094,
    "sentiment_recall": 0.36802811867032054,
    "sentiment_accuracy": 0.3012884043607532,
    "total_examples": 1009
  },
  {
    "model": "llama3.3:70b",
    "n_shots": 5,
    "target_macro_f1": 0.50294581020838,
    "target_precision": 0.565967365967366,
    "target_recall": 0.5275256127609909,
    "target_accuracy": 0.6798810703666998,
    "sentiment_macro_f1": 0.3608732016126721,
    "sentiment_precision": 0.5619058246150098,
    "sentiment_recall": 0.42313326581675204,
    "sentiment_accuracy": 0.3587710604558969,
    "total_examples": 1009
  },
  {
    "model": "llama3.2:latest",
    "n_shots": 0,
    "target_macro_f1": 0.3267257443709364,
    "target_precision": 0.4950022336385973,
    "target_recall": 0.4975128295385072,
    "target_accuracy": 0.34985133795837464,
    "sentiment_macro_f1": 0.18026977822746698,
    "sentiment_precision": 0.5510135135135135,
    "sentiment_recall": 0.2645940069105207,
    "sentiment_accuracy": 0.3022794846382557,
    "total_examples": 1009
  },
  {
    "model": "llama3.2:latest",
    "n_shots": 1,
    "target_macro_f1": 0.4578461223549881,
    "target_precision": 0.4661084529505582,
    "target_recall": 0.4612219093317524,
    "target_accuracy": 0.5004955401387512,
    "sentiment_macro_f1": 0.15824889051137983,
    "sentiment_precision": 0.17563125728760404,
    "sentiment_recall": 0.2172992374597879,
    "sentiment_accuracy": 0.20118929633300298,
    "total_examples": 1009
  },
  {
    "model": "llama3.2:latest",
    "n_shots": 3,
    "target_macro_f1": 0.42348220099126993,
    "target_precision": 0.43293734072852647,
    "target_recall": 0.48342349519239675,
    "target_accuracy": 0.6590683845391476,
    "sentiment_macro_f1": 0.16840985304703104,
    "sentiment_precision": 0.2716496149965791,
    "sentiment_recall": 0.25669486476825926,
    "sentiment_accuracy": 0.199207135777998,
    "total_examples": 1009
  },
  {
    "model": "llama3.2:latest",
    "n_shots": 5,
    "target_macro_f1": 0.5056168764643523,
    "target_precision": 0.5121004566210046,
    "target_recall": 0.5096962595179428,
    "target_accuracy": 0.6184340931615461,
    "sentiment_macro_f1": 0.25406797121831454,
    "sentiment_precision": 0.3570405664747535,
    "sentiment_recall": 0.31323245561777674,
    "sentiment_accuracy": 0.2705649157581764,
    "total_examples": 1009
  }
]